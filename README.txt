CS 470 Machine Learning HW3 
Decision Tree Classifier for Heart Disease Prediction

Honor Code Statement:
THIS CODE IS MY OWN WORK, IT WAS WRITTEN WITHOUT CONSULTING CODE WRITTEN BY OTHER STUDENTS. Asuka Li 

Libraries used: pandas, numpy, Counter from collections, math, and re

To run the file DecisionTreeClassifier, you will need to run the cells from top to bottom (1st cell, 2nd, and the 3rd cell in the order). In the last cell, when calling the method genDecisionTree(), the first parameter is the dataset. You will input the name of the csv file on the fist line of the third cell (in the double quotations of pd.read_csv) to read the data. The second parameter of genDecisionTree method is the text file with personIDs of the training set, and the third parameter the personIDs of the testing set. Please write the exact name of the text files within the quotations. The last parameter of the method is the name of the file it will output the predicted results. 

I used Gini index to evaluate the impurity measure in this decision tree classifier algorithm. Gini impurity ranges from 0 to 0.5, while entropy ranges from 0 to 1, from which we can conclude that Gini impurity requires less computational power. In addition to this, my implemented algorithm evaluates what the best split among attributes and values with the impurity gain derived from the change of Gini scores of the parent node and its children nodes. With the utilization of Gini impurity change, the algorithm is able to detect the best attribute to split more accurately. As described above, I conducted the Gini impurity for each midpoint values of the best splitting attribute to decide on the best splitting value for numeric continuous attributes. For categorical attributes, I used two-way splitting method to evaluate which value is the best to split with. In these attributes, they're split into one way with the splitting value and another way with values unequal to the splitting value. I utilized tree with my own Node class with two children nodes to implement the splitting value portion of the decision tree. Each node stores multiple variables (depth, rule to split, which attribute it's using to split, etc.) to help with generating the decision tree. I included max_depth and min_samples_split variables to optimize the algorithm. Max_depth specifies the maximum depth of the decision for the algorithm to stop when reached to avoid unnecessary tedious splitting. Min_samples_split serves similar purposes by limiting the number of rows the current node contains to move onto the next splitting point. With this optimization technique, some leaf nodes are returned with majority class even if there are unused attribute or not all records belong to the same class. 

